# GCP Variant Transforms

[![Build Status](https://travis-ci.org/googlegenomics/gcp-variant-transforms.svg?branch=master)](https://travis-ci.org/googlegenomics/gcp-variant-transforms)
[![Coverage
Status](https://coveralls.io/repos/github/googlegenomics/gcp-variant-transforms/badge.svg)](https://coveralls.io/github/googlegenomics/gcp-variant-transforms)

## Overview

This is a tool for transforming and processing
[VCF](https://samtools.github.io/hts-specs/VCFv4.3.pdf) files in a scalable
manner based on [Apache Beam](https://beam.apache.org/) using 
[Dataflow](https://cloud.google.com/dataflow/) on Google Cloud Platform.

It can be used to directly load VCF files to
[BigQuery](https://cloud.google.com/bigquery/) supporting hundreds of thousands
of files, millions of samples, and billions of records.

### Prerequisites

1.  Setup a [Google Cloud account](https://cloud.google.com/) and
    [create a project](https://cloud.google.com/resource-manager/docs/creating-managing-projects).
1.  [Sign up and install the Google Cloud SDK](https://cloud.google.com/genomics/install-genomics-tools)
1.  Enable the [Genomics, Compute Engine, Cloud Storage, and Dataflow APIs](https://console.cloud.google.com/flows/enableapi?apiid=genomics,storage_component,storage_api,compute_component,dataflow)
1.  Open the [billing](https://console.cloud.google.com/project/_/settings) page
    for the project you have selected or created, and click _Enable billing_.
1.  Create a new BigQuery dataset by visiting the
    [BigQuery web UI](https://bigquery.cloud.google.com/), clicking on the
    down arrow icon next to your project name in the navigation, and clicking on
    _Create new dataset_.

## Loading VCF files to BigQuery

### Using docker

The easiest way to run the VCF to BigQuery pipeline is to use the
[docker](https://www.docker.com/) image and run it with the
[Google Genomics Pipelines API](https://cloud-dot-devsite.googleplex.com/genomics/pipelines)
as it has the binaries and all dependencies pre-installed.

First, set up the pipeline configurations shown below and save it as
`vcf_to_bigquery.yaml`. The parameters that you need to replace are:

* `my_project`: This is your project name that contains the BigQuery dataset.
* `gs://my_bucket/vcffiles/*.vcf`: A location in Google Cloud Storage where the
  VCF file are stored. You may specify a single file or provide a pattern to
  load multiple files at once. Please refer to the
  [Variant Merging](docs/variant_merging.md) documentation if you want
  to merge samples across files. The pipeline supports gzip, bzip, and
  uncompressed VCF formats. However, it runs slower for compressed files as they
  cannot be sharded.
* `my_bigquery_dataset`: Your BigQuery dataset to store the output.
* `my_bigquery_table`: This can be any ID you like (e.g. vcf_test).
* `gs://my_bucket/staging` and `gs://my_bucket/temp`: These can be any folder in
  Google Cloud Storage that your project has write access to. These are used to
  store temporary files needed for running the pipeline.

```yaml
name: vcf-to-bigquery-pipeline
docker:
  imageName: gcr.io/gcp-variant-transforms/gcp-variant-transforms
  cmd: |
    ./opt/gcp_variant_transforms/bin/vcf_to_bq \
      --project my_project \
      --input_pattern gs://my_bucket/vcffiles/*.vcf \
      --output_table my_project:my_bigquery_dataset.my_bigquery_table \
      --staging_location gs://my_bucket/staging \
      --temp_location gs://my_bucket/temp \
      --job_name vcf-to-bigquery \
      --runner DataflowRunner
```

Next, run the following command to launch the pipeline. Replace `my_project`
with your project name, `gs://my_bucket/temp/runner_logs` with a Cloud Storage
folder to store the logs from the pipeline.

```bash
gcloud alpha genomics pipelines run \
    --project my_project \
    --pipeline-file vcf_to_bigquery.yaml \
    --logging gs://my_bucket/temp/runner_logs \
    --zones us-west1-b \
    --service-account-scopes https://www.googleapis.com/auth/bigquery
```

Please note the operation ID returned by the above script. You can track the
status of your operation by running:

```bash
gcloud alpha genomics operations describe <operation-id>
```

The returned data will have `done: true` when the operation is done. 
A detailed description of the Operation resource can be found in the
[API documentation](https://cloud.google.com/genomics/reference/rest/v1/operations)

The underlying pipeline uses
[Cloud Dataflow](https://cloud.google.com/dataflow/). You can navigate to the
[Dataflow Console](https://console.cloud.google.com/dataflow), to see more
detailed view of the pipeline (e.g. number of records being processed, number of
workers, more detailed error logs).

### Running from github

In addition to using the docker image, you may run the pipeline directly from
source. First install git, python, pip, and virtualenv:

```bash
sudo apt-get install -y git python-pip python-dev build-essential
sudo pip install --upgrade pip
sudo pip install --upgrade virtualenv
```

Run virtualenv, clone the repo, and install pip packages:

```bash
virtualenv venv
source venv/bin/activate
git clone https://github.com/googlegenomics/gcp-variant-transforms.git
cd gcp-variant-transforms
pip install --upgrade .
# Workaround needed until Beam 2.3.0 is released. See Issue #65.
pip install --upgrade apache_beam[gcp]
```

You may use the
[DirectRunner](https://beam.apache.org/documentation/runners/direct/)
(aka local runner) for small (e.g. 10,000 records) files or
[DataflowRunner](https://beam.apache.org/documentation/runners/dataflow/)
for larger files. Files should be stored on Google Cloud Storage if using
Dataflow, but may be stored locally for DirectRunner.

Example command for DirectRunner:

```bash
python -m gcp_variant_transforms.vcf_to_bq \
  --input_pattern gcp_variant_transforms/testing/data/vcf/valid-4.0.vcf \
  --output_table projectname:bigquerydataset.tablename
```

Example command for DataflowRunner:

```bash
python -m gcp_variant_transforms.vcf_to_bq \
  --input_pattern gs://my_bucket/vcffiles/*.vcf \
  --output_table my_project:my_bigquery_dataset.my_bigquery_table \
  --project my_project \
  --staging_location gs://my_bucket/staging \
  --temp_location gs://my_bucket/temp \
  --job_name vcf-to-bigquery \
  --setup_file ./setup.py \
  --runner DataflowRunner
```

## Additional topics

* [Understanding the BigQuery Variants Table Schema](docs/bigquery_schema.md)
* [Variant merging](docs/variant_merging.md)
* [Handling large inputs](docs/large_inputs.md)
* [Appending data to existing tables](docs/data_append.md)

## Development

[Development Guide](docs/development_guide.md)
